---
title: "Homework 4 - Answer Key"
author: "Melinda Higgins"
date: "October 30, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

## 1. Perform a Simple Linear Regression for:

- OUTCOME variable `cesd`: "Center for Epidemiological Studies-Depression (CESD) total score - Baseline"
- PREDICTOR variable `indtot`: "Inventory of Drug Use Consequences (InDue) total score - Baseline"
- decide if you want to transform either variable `cesd` or `indtot` and if so, what transformation you applied and why - you can also decide not to transform (i.e. tradeoffs between model fit and interpretability of your results) - discuss your reasoning.
    
## 1. Answer

Here is the code and output after running a simple linear regression using `lm()` function.
    
```{r}
library(tidyverse)
library(haven)

helpdat <- haven::read_spss("helpmkh.sav")

# create subset
# select indtot, cesd and racegrp

h1 <- helpdat %>%
  select(indtot, cesd, racegrp)

# run simple linear regression
# using the lm
# save the results in the fit1 object
fit1 <- lm(cesd ~ indtot, data=h1)
```

### Model summary

```{r}
# look at a summary() of the model
summary(fit1)
```

### Possible Transformation

I probably would not do a transformation since the residuals look fairly normal (see diagnostic plots below), but you could run the `spreadLevelPlot()` function from the `car` package to see if a power  transformation is suggested. This suggests a power transformation of 1.639 which could be rounded up to 2. This is optional and not needed for this data.

```{r}
library(car)
# look at the spreadLevelPlot
# this also provides a suggestion of 
# possible power transformation
car::spreadLevelPlot(fit1)
```

Plus the `gvlma()` function from the `gvlma` package can be run to check model assumptions, which also all look ok.

```{r}
# global test of linear model assumptions
# install gvlma package
library(gvlma)
gvmodel <- gvlma::gvlma(fit1)
summary(gvmodel)
```

## 2. Perform regression diagnostics:

- check the normality of the residuals (histogram and Q-Q plots)
- check for linearity - is there any systematic relationship between the residuals and the predicted (or fitted) values?
- homoscedasticity - plot of standardized residuals versus fitted values - this is known as a "Scale-Location" graph.
- check for outliers and data points with high leverage or influence: outliers are often identified with standardized residuals > 3 (or <-3) and influential observations are often identified using Cook's D
    
## 2. Answer

A good set of 4 diagnostic plots can be obtained using the `plot()` function for the fitted model output.

### Diagnostic plots

These 4 diagnostic plots show:

* residuals vs fitted values - the line here is flat and shows no obvious trend, but the data do cluster on the higher end of the fitted values than the lower end, indicating some skewness
* normal Q-Q plot of the residuals - this plot looks fairly linear indicating a close to normal distribution
* scale-location plot - there is a slight trend downwards, but this slope is minor - and in general the variability looks pretty consistent across all of the fitted values (no obvious heteroscedasticity)
* the last plot of Cook's distance does highlight a few possible outliers - cases 1, 3 and 6, in the but these appear to be minor as they are not obvious in the Q-Q plot nor in the histogram
    
```{r}
# get diagnostic plots
par(mfrow=c(2,2))
plot(fit1)

# reset par
par(mfrow=c(1,1))
```

### Histogram of the Residuals

The histogram of the residuals look normal. No skewness and no obvious outliers.

```{r}
# histogram of the residuals
hist(fit1$residuals)
```

### Scatterplot of Data with Fitted Line

There is quite a bit of scatter in the plot with a lot of variability in both `indtot` and `cesd`, so a linear fit line looks as good as any - there is no obvious curvature to the data. 

```{r}
plot(h1$indtot, h1$cesd)
abline(lm(h1$cesd ~ h1$indtot))
```

## 3. Provide a summary of the regression results.

- provide a **FIGURE** of the model, in this case a scatterplot with the fitted line overlaid and 95% confidence intervals if you can
- Make a **TABLE** presenting the fitted regression model (coefficients and tests of significance for those coefficients)
- describe the variance explained by the model (based on r2)
- describe the model itself based on the y-intercept and slope terms
- note any limitations or issues with the model fit or interpretation of the model
    
### Model fit with 95% confidence intervals for fitted line

```{r}
# using a ggplot2 approach
ggplot(h1, aes(indtot, cesd)) +
  geom_point() +
  stat_smooth(method = lm)
```

### Table of the model fit

```{r, results = "asis"}
library(knitr)
library(xtable)
print(xtable(summary(fit1)), type = "html")
```

### Variance explained

The variablity explained by this model is `r (summary(fit1)$adj.r.squared)*100`%, which is the adjusted r2 captured as `summary(fit1)$adj.r.squared`.

### Describe the model

For `indtot` scores equal to 0, the model estimates that a subject would have a `cesd` score of `r coef(fit1)[1]` given the y-intercept. For each 1 point increase in `indtot` scores, the `cesd` score will increase on average by `r coef(fit1)[2]` based on the slope estimate.

### Model fit and Any Other issues

It is worth noting that given the wide variability in both the `indtot` and `cesd` a linear fit line indicates a weak positive correlation between these 2 variables, but the residuals do show wide variation about the best fitted line, indicating that the linear trend is weak at best.
    
## 4. Perform a One-way ANOVA for:

- OUTCOME variable `cesd`: "Center for Epidemiological Studies-Depression (CESD) total score - Baseline"
- GROUP variable `racegrp`: "Racial Group of Respondent"
- options - you can use either an ANOVA or GLM modeling approach
- if the GROUP variable is significant, also perform _post hoc_ tests - use some kind of pairwise error rate adjustment (i.e. bonferroni, sidak, Tukey's HSD, etc) - be sure to report which one you used and why

### ANOVA Model Results with dummy coding
    
```{r}
# one-way ANOVA
# we can use the lm() function
# it does "dummy" coding on the fly
# run racegrp as either the character
# type or as a factor - either will work
fit2.lm <- lm(cesd ~ racegrp, data=h1)
summary(fit2.lm)
```

### ANOVA results for group effect overall

```{r}
# the aov() function
# gives the global test for the "group" effect
fit2.aov <- aov(cesd ~ racegrp, data=h1)
summary(fit2.aov)
```

Yes, `racegrp` is significant - there are significant differences between the 4 races.

### Post hoc tests

Since `racegrp` was significant, let's run all of the pairwise comparisons. The code below will show the options for adjusting the error-rate due to multiple pairwise comparisons including: Bonferroni, Holm, and Tukey HSD.

```{r}
# post hoc tests
# Tukey HSD
TukeyHSD(fit2.aov)

# using Bonferroni error-rate correction
pairwise.t.test(h1$cesd, h1$racegrp, p.adj = "bonf")

# using the Holm error-rate correction
pairwise.t.test(h1$cesd, h1$racegrp, p.adj = "holm")
```

These pairwise comparisons show that there are significant differences between white-black, but none of the other pairwise comparisons were significant.

## 5. Perform model diagnostics:

- homoscedasticity - look at a test for equal variance (Levene's test or Bartlett's test or equivalent).
- if this test of equal variances fails, you may want to report a modified F-test (e.g. Welch's test)

### Test of Equal Variances

```{r}
# barlett's test for homogenity of variances
# note: put the formula back in
bartlett.test(cesd ~ racegrp, data=h1)
```

This was not significant, so we can report the usual F-statistic test for `racegrp` rather than the "robust" Welch's F-test.

## 6. Present a summary of the ANOVA results.

- Make a **FIGURE** of the group mean differences - either an error-bar plot or a series of boxplots one for each group to show the group differences in the outcome
- Make a **TABLE** presenting the ANOVA results
- describe the model results - was the GROUP (`racegrp`) significant?
- If GROUP is significant, what did the post hoc tests reveal?

### Plot of Means and 95% Confidence Intervals 

```{r}
# get a means plot using
# plotmeans() from gplots package
library(gplots)
gplots::plotmeans(cesd ~ racegrp, 
                  data=h1)
```

### Table of the ANOVA results - overall group effect

`racegrp` was significant - no significant diff

```{r, results = "asis"}
print(xtable(summary(fit2.aov)), 
      type = "html")
```




